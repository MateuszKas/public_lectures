{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".rendered_html table, .rendered_html th, .rendered_html tr, .rendered_html td {\n",
       "     font-size: 90%;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".rendered_html table, .rendered_html th, .rendered_html tr, .rendered_html td {\n",
    "     font-size: 90%;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Analiza i bazy danych\n",
    "\n",
    "## Wstęp\n",
    "\n",
    "### dr hab. inż. Jerzy Baranowski, Prof. AGH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Informacje ogólne\n",
    "- Katedra Automatyki i Robotyki, C3, p. 214\n",
    "- Konsultacje \n",
    "    - Poniedziałki 9:00-10:00\n",
    "- jb@agh.edu.pl\n",
    "- wykłady dostępne tutaj: https://github.com/KAIR-ISZ/public_lectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Organizacja przedmiotu\n",
    "- Wykład:\n",
    " - 18 godzin lekcyjnych z analizy danych, \n",
    " - 10 godzin lekcyjnych z baz danych. \n",
    " - wykłady na Youtube z premierami na żywo,\n",
    "- Laboratorium:\n",
    " - 3 tematy z analizy danych,\n",
    " - 3 tematy z baz danych.\n",
    " - 1 temat integracyjny\n",
    "- Zaliczenie:  \n",
    " - praca na zajęciach oceniana z checklistą."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Co my tu robimy?\n",
    "**Big data** - zbiory danych, których nie da się ogarnąć bazą danych.\n",
    "\n",
    "W ramach tego przedmiotu zajmujemy się resztą, czyli danymi w plikach (csv, ...) lub w bazach danych. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analiza danych\n",
    "\"Analiza danych jest interdyscyplinarną praktyką opartą na metodach z inżynierii danych, statystyki opisowej, eksploracji danych, uczenia maszynowego i analiz predykcyjnych. Podobnie jak badania operacyjne, analiza danych koncentruje się na wdrażaniu decyzji opartych na danych i zarządzaniu ich konsekwencjami\" - *Practical Data Science with R* N. Zumel, J. Mount\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Podejście projektowe\n",
    "![](img/lifecycle.png)\n",
    "*Practical Data Science with R* - N. Zumel, J. Mount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Najpopularniejsze narzędzie analizy danych\n",
    "\n",
    "<img src=\"img/excel365.svg\" alt=\"drawing\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Analiza w excelu zgubiła 16 tys wyników testów na COVID\n",
    "![](img/excel_covid.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Inne ciekawe przygody w Excelu\n",
    "\n",
    "- JPMorgan Chase - 6.5 miliarda dolarów przez kopiuj wklej\n",
    "- Eastman Kodak - odprawy zawyżone o 11 milionów dolarów, konsekwencja spadek cen akcji\n",
    "- US Federal National Mortgage Association - 1.136 miliarda dolarów niezgodności\n",
    "- Fidelity Investment - pomyłka w znaku na 1.3 miliarda dolarów\n",
    "- Human Genome Project - konieczność zmian nazw genów, w ok. 20% badań z genetyki Excel zamieniał część genów z sekwencji na daty\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Narzędzia do analizy danych\n",
    "\n",
    "- Oprogramowanie, które zapewnia narzędzia analizy potrzebne w ramach tego kursu:\n",
    " - R + Tidyverse\n",
    " - **Python + pandas + Matplotlib + statsmodels** (+ może coś później)\n",
    "- Oczywiście są inne rzeczy:\n",
    " - STATA\n",
    " - Statistica\n",
    " - SAS\n",
    " - IBM Watson  \n",
    " \n",
    "ale nie będziemy o nich rozmawiać.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kryzys replikacji \n",
    "\n",
    "\"Kryzys replikacji (lub kryzys odtwarzalności, eng *replication crisis*) jest ciągłym kryzysem metodologicznym, w którym stwierdzono, że wiele badań naukowych jest trudnych lub niemożliwych do odtworzenia lub odtworzenia. Kryzys replikacji ma największy wpływ na nauki społeczne i przyrodnicze.\" - Wikipedia, za Schooler, J. W. (2014). \n",
    "*Metascience could rescue the 'replication crisis'* . Nature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Konsekwencja\n",
    "Jeżeli nie da się powtórzyć wyników badań naukowych, które są publikowalne, to kto wie ile raportów wewnętrznych jest absolutnie niepowtarzalnych i nieodtwarzalnych?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Project TIER\n",
    "\n",
    "Project TIER (*Teaching Integrity in Empirical Research*) ma na celu promowanie włączenia zasad związanych z przejrzystością i powtarzalnością do szkolenia analityków danych. Wymyślony dla nauk społecznych ale stosowalny wszędzie. \n",
    "\n",
    "https://www.projecttier.org/\n",
    "\n",
    "Jest to zbiór pryncypiów, określających jak powinien wyglądać proces pracy nad projektem analizy danych tak aby był on replikowalny przez innych (lub też przez autora po latach).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Jak to działa?\n",
    "TIER Protocol proponuje rozpatrywanie każdego projektu analizy danych w formie trzech etapów:\n",
    "1. Przed danymi (*pre-data*)\n",
    "2. Pracę z danymi (*data work*)\n",
    "3. Podsumowanie (*wrap-up*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pre-data\n",
    "Najprostsza faza. Wymaga jedynie\n",
    "1. Utworzenia struktury katalogów\n",
    "\n",
    "2. Utworzenia pustych dokumentów.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Struktura katalogów\n",
    "![](img/TIER-folder.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Puste dokumenty\n",
    "Należy utworzyć\n",
    "- plik readme,\n",
    "- przewodnik metadanych,\n",
    "- dodatek danych.\n",
    "\n",
    "i na początek je zatytułować."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data work\n",
    "Porządkuje przede wszystkim to co robimy przed rozpoczęciem zasadniczej analizy:\n",
    "1. Udokumentowanie oryginalnych danych.\n",
    "2. Przygotowanie kodu przetwarzającego dane.\n",
    "3. Utworzenie załącznika danych (data appendix).\n",
    "4. Przygotowanie kodu realizującego wymagane rezultaty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Udokumentowanie oryginalnych danych\n",
    "Jest niezwykle ważne aby trzymać zawsze kopię oryginalnych danych. Podobnie kluczowe jest przechowywanie metadanych, które pozwalają zrozumieć co w danych się dzieje. Jest to niezbędne dla odtworzenia procesu decyzyjnego. \n",
    "\n",
    "1. Zapisać niezmienioną formę danych w folderze z oryginalnymi danymi.\n",
    "2. Opisać plik danych w przewodniku metadanych (zgodnie ze specyfikacją protokołu).\n",
    "3. Utworzenie importowalnego pliku i opisanie jak on powstał w pliku readme. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Przetwarzanie danych\n",
    "Ten krok ma na celu przygotowanie danych wykorzystywanych do analizy. Tutaj podejmowane są pierwsze decyzje wpływające na wynik analizy. Obejmuje to między innymi:\n",
    "- usuwanie błędów i różnic między danymi,\n",
    "- usuwanie niepotrzebnych danych,\n",
    "- łączenie danych z różnych źródeł,\n",
    "- transpozycje,\n",
    "- tworzenie nowych zmiennych,\n",
    "- imputacja brakujących wartości,\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Przetwarzanie danych cd\n",
    "\n",
    "\n",
    "W tym kroku należy też zdecydować się na sposób organizacji przetwarzania danych w pliki. Musi on zostać następnie opisany w pliku readme.\n",
    "\n",
    "Kod i dane muszą być zapisane w odpowiednich katalogach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data appendix\n",
    "Dodatek do danych jest książką kodów i przewodnikiem użytkownika dla utworzonych plików do analizy danych. Często pomijany krok, który sprawia, że odtwarzanie pracy nie jest możliwe. Powinien powstać przed rozpoczęciem samej analizy.\n",
    "\n",
    "Dodatek do danych powinien zawierać informacje o każdej zmiennej w plikach danych analizy, w tym definicje i kodowanie (dla wszystkich zmiennych), statystyki podsumowujące i histogramy (dla zmiennych ilościowych) oraz tabele i wykresy względnej częstotliwości (dla zmiennych kategorialnych).\n",
    "\n",
    "Należy przygotować skrypty, które wygenerują wszystkie dane do dodatku. Kod i dokument zapisać w odpowiednich folderach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Zasadnicza analiza danych\n",
    "Bardzo istotne jest, aby jednoznacznie zidentyfikować pliki, które wygenerują interesujące nas rezultaty. W tych plikach dane nie powinny już być przetwarzane. Jeżeli konieczne jest dalsze przetwarzanie należī zmodyfikować fazę pre processingu.\n",
    "\n",
    "Struktura plików zależy oczywiście od skali i zakresu projektu. Niezależnie od tego, plik readme powinien opisywać w jakiej kolejności powinny być wywoływane skrypty by uzyskać rezultaty. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Wrap up\n",
    "\n",
    "1. Skończyć plik readme\n",
    "2. Przeczytanie i poprawianie kodu.\n",
    "3. Sprawdzenie kompletności."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Plik readme\n",
    "Dotychczas sukcesywnie uzupełniany. Należy go zakończyć opisem struktury katalogów (jakie mamy pliki co one robią).\n",
    "\n",
    "Dodatkowo czytelna instrukcja krok po kroku pozwalająca zreplikować analizę."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Przeczytanie i poprawianie kodu\n",
    "W ramach projektu powstał kod, odpowiadający za dodatek do danych, pre-processing i calą analizę. Należy upewnić się czy się on uruchamia, czy jest on zrozumiały (komentarze). To jest dobry moment na jakąś drobną optymalizację oraz usuwanie niepotrzebnych rzeczy, które powstawały w trakcie analizy. (Pamiętajmy o repozytorium!).\n",
    "\n",
    "Należy też sprawdzić czy instrukcja z pliku readme działa, jeżeli nie trzeba poprawić ją albo kod."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sprawdzenie kompletności\n",
    "\n",
    "1. Należy sprawdzić czy wszystkie pliki wymagane przez specyfikacje są we właściwych miejscach. Tu jest dobry moment by się upewnić czy raport z projektu znajduje się w katalogu z dokumentacją.\n",
    "2. Sprawdzić, czy zawartość plików jest zgodna ze specyfikacją protokołu.\n",
    "2. Usunąć wszystkie zbędne pliki."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}